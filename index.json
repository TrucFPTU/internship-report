[{"uri":"https://trucfptu.github.io/internshiop-report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"AWS Site-to-Site VPN now supports IPv6 on external IP addresses By Ruskin Dantra, SaiJeevan Devireddy, and Scott Morrison\nSeptember 24, 2025 – AWS Site-to-Site VPN, Networking \u0026amp; Content Delivery\nAmazon Web Services (AWS) Site-to-Site VPN is a fully managed service that enables you to create a secure connection between your data center or branch office and your AWS resources using IP Security (IPSec) tunnels. It provides critical connectivity for many types of workloads, including connecting on-premises workloads to the cloud, connecting devices to the cloud, and enabling encrypted communication.\nSite-to-Site VPN is a flexible option for teams that need fast connectivity. When a VPN tunnel is configured, you must choose both external and internal IP addresses. IPv4 was the only option for both external and internal IP addresses until 2018, when AWS introduced support for IPv6 for internal IP addresses. In July 2025, AWS expanded this capability by announcing support for assigning IPv6 addresses to external IPs.\nYou can now connect IPv6 networks using Site-to-Site VPN connectivity with native IPv6 addresses, eliminating the need for IPv6 → IPv4 → IPv6 translation.\nBenefits of IPv6 support Migration to IPv6\nThis announcement enables a more straightforward transition to IPv6, allowing users to connect two IPv6 networks using only IPv6.\nIPv4 address exhaustion\nUsing IPv6 for outer tunnels helps conserve valuable IPv4 addresses in an environment where IPv4 has become scarce.\nCost reduction\nWith IPv6 on external IPs, you can reduce the cost of connecting two networks without paying for public IPv4 addresses on either side of the tunnels (AWS side and on-premises side).\nRegulatory and compliance requirements\nFor regulated industries such as government, healthcare, finance, and telecommunications, IPv6 external addresses make it easier to meet requirements for IPv6-only networks.\nSupported configurations IPv6 is supported only when Site-to-Site VPN terminates on an AWS Transit Gateway or an AWS Cloud WAN core network edge (CNE). Figure 1 shows an example of an IPv6 Site-to-Site VPN terminating on a Transit Gateway.\nFigure 1: IPv6-only VPN connection to a Transit Gateway\nTerminating an IPv6 Site-to-Site VPN on a CNE follows the same model as shown above.\nIPv6 Site-to-Site VPN overview To get started with an IPv6 Site-to-Site VPN, download the provided AWS CloudFormation template and deploy it in the AWS Region of your choice. The template requires only one parameter: an email address for notifications about potential outages. The template deploys the infrastructure shown in Figure 2 into your account.\nFigure 2: IPv6 Site-to-Site VPN CloudFormation deployment environment\nDeploying this template incurs costs, which are described in the Cost section. Refer to Cleaning up for teardown instructions. In the next sections, we describe the deployed infrastructure in detail.\nSimulated enterprise data center To simulate an enterprise data center, the template deploys a VPC with the following infrastructure:\nAn Amazon VPC configured as dual stack with one IPv4 prefix and one IPv6 prefix. The VPC IPv6 prefix can vary depending on whether you use AWS-provided IPv6 or Bring Your Own IP (BYOIP). An internet gateway attached to the VPC. This provides the VPN underlay and allows installation and configuration of the Amazon EC2 VPN router that simulates your on-premises router. Two IPv6-only public subnets that host the Amazon EC2 VPN router. An EC2 instance deployed in the IPv6-only public subnet to simulate the VPN router. Two public dual-stack subnets. Two NAT gateways deployed in those subnets to perform NAT64, allowing direct connectivity to the EC2 VPN router using AWS Systems Manager Session Manager. The NAT gateways are also required for CloudFormation to receive completion signals from cfn-* hooks. Two private dual-stack subnets for VPC endpoints that access other AWS services through AWS PrivateLink. Two IPv6-only private subnets to host test instances. An EC2 instance deployed in an IPv6-only private subnet to simulate a test instance. AWS infrastructure The template terminates the IPv6 Site-to-Site VPN at a Transit Gateway and deploys the following infrastructure:\nA dual-stack Amazon VPC with one IPv4 prefix and one IPv6 prefix. Two private dual-stack subnets to host Transit Gateway attachments. A Transit Gateway with IPv6 support enabled. The VPC attached to the Transit Gateway, associated with the default route table, and configured to propagate routes to the default route table. Two IPv6-only private subnets that host test instances. An EC2 instance deployed in an IPv6-only private subnet to simulate a test instance. Two private dual-stack subnets that host VPC endpoints for accessing AWS services through AWS PrivateLink. An EC2 instance deployed in a dual-stack private subnet that acts as a jump box. This is required because this VPC does not have a NAT gateway for NAT64. Shared infrastructure AWS Secrets Manager for managing the VPN pre-shared key. Amazon CloudWatch for logs, metrics, and alarms. An Amazon Simple Notification Service (Amazon SNS) topic for outage notifications, such as tunnel-down events. An AWS Key Management Service (AWS KMS) customer managed key (CMK) to encrypt CloudWatch logs, Secrets Manager secrets, and the SNS topic. VPN infrastructure The template also deploys the following VPN connectivity resources to build an IPv6 Site-to-Site VPN between the simulated on-premises data center and AWS:\nA customer gateway that logically represents your on-premises router. An IPv6 Site-to-Site VPN connection that terminates on the Transit Gateway. Configuration of the EC2 instance that simulates the router. IPv6 Site-to-Site VPN walkthrough The following steps walk you through the IPv6 Site-to-Site VPN configuration.\nPrerequisites Ensure the following before proceeding:\nThe CloudFormation template has deployed successfully with no errors. Step 1: Customer gateway Open the Amazon VPC console and navigate to\nVirtual private network (VPN) → Customer gateways.\nFigure 3: Navigate to customer gateways\nA customer gateway is already deployed for you. Creating a customer gateway for an IPv6 Site-to-Site VPN follows the same steps as for IPv4. The only difference is that it points to the IPv6 address of your router. In this case, it points to the IPv6 address of the EC2 instance simulating the VPN router.\nFigure 4: AWS VPC Console – Customer Gateway view\nNavigate to the Amazon EC2 console, open Instances, and locate the instance named\n…-onprem-vpn/vpn-router-instance. Its associated IPv6 address is displayed.\nFigure 5: EC2 VPN router with IPv6 address\nStep 2: IPv6 Site-to-Site VPN The template also deploys an IPv6 Site-to-Site VPN connection.\nFigure 6: IPv6 Site-to-Site VPN details\nOpen the Amazon VPC console. In the navigation pane, choose Virtual private network (VPN) → Site-to-Site VPN connections. Select the VPN connection created by the template named …-vpn-connection. This connection has an associated IPv6 customer gateway address. Navigate to the Tunnel details tab: Two IPv6 outside IP addresses, one per tunnel. Two /126 IPv6 inside IP address prefixes. These addresses are allocated from the fd00::/8 range. The first usable IPv6 address is on the AWS side. The second usable IPv6 address is on your side. Example: if the inside IPv6 prefix is fd00:…:cd90/126: AWS side: fd00:…:cd91 On-premises side: fd00:…:cd92 IPv6 Site-to-Site VPN uses dynamic routing and advertises on-premises prefixes to AWS.\nYou can observe the on-premises route (2001:db8:0:0::/56) propagated into the Transit Gateway route table via the Site-to-Site VPN attachment.\nFigure 7: On-premises route propagated to the Transit Gateway route table\nVerify routes on the AWS side Open the Amazon VPC console. In the navigation pane, choose Transit gateways → Transit gateway route tables. Select the route table associated with the Transit Gateway created by the stack. Use the CloudFormation output AwsTransitGatewayId to confirm the correct Transit Gateway. Verify routes on the on-premises side Open the Amazon EC2 console. Select the instance named …-onprem-vpc/vpn-router-instance. Choose Connect, then use Session Manager to connect. Run the following command: sudo vtysh -c \u0026#34;show ipv6 route bgp\u0026#34; "},{"uri":"https://trucfptu.github.io/internshiop-report/3-blogstranslated/3.3-blog3/","title":"Blog 2","tags":[],"description":"","content":"Optimizing the Amazon EMR runtime for Apache Spark with EMR S3A By Giovanni Matteo Fumarola, Narayanan Venkateswaran, Syed Shameerur Rahman, Sushil Kumar Shivashankar, and Rajarshi Sarkar – September 24, 2025 – Amazon EMR, Analytics, Best Practices, Intermediate (200), Permalink\nWith the Amazon EMR 7.10 runtime, Amazon EMR introduced EMR S3A, an enhanced version of the open-source S3A file system connector. This enhanced connector is now automatically configured as the default S3 file system connector for Amazon EMR deployment options, including:\nAmazon EMR on EC2 Amazon EMR Serverless Amazon EMR on Amazon EKS Amazon EMR on AWS Outposts while maintaining full compatibility with the open-source Apache Spark APIs.\nIn the Amazon EMR 7.10 runtime for Apache Spark, the EMR S3A connector delivers read performance comparable to EMRFS, as demonstrated by the TPC-DS query benchmark. The most significant performance improvements are observed for write workloads, with gains of:\n7% for static partition overwrites 215% for dynamic partition overwrites compared to EMRFS In this post, we present the improved read and write performance benefits of using Amazon EMR 7.10.0 runtime for Apache Spark with EMR S3A, compared with EMRFS and the open-source S3A file system connector.\nRead performance comparison To evaluate read performance, we used a test environment based on Amazon EMR runtime version 7.10.0 running:\nApache Spark 3.5.5 Apache Hadoop 3.4.1 The test infrastructure consisted of an Amazon EC2 cluster with nine r5d.4xlarge instances:\nPrimary node: 16 vCPUs and 128 GB of memory Eight core nodes: a total of 128 vCPUs and 1024 GB of memory The performance evaluation was conducted using a comprehensive benchmarking methodology to ensure accurate and meaningful results.\nThe input dataset was selected with a TPC-DS scale factor of 3 TB, containing 17.7 billion records, equivalent to approximately 924 GB of compressed data, partitioned in Parquet file format.\nSetup instructions and technical details are available in the associated GitHub repository. We used Spark’s in-memory data catalog to store metadata for the TPC-DS databases and tables.\nTo ensure a fair and accurate comparison between EMR S3A, EMRFS, and the open-source S3A implementation, we followed a three-phase benchmarking approach:\nPhase 1: Baseline performance Establish baseline performance using the default Amazon EMR configuration with the EMR S3A connector Create a reference point for subsequent comparisons Phase 2: EMRFS analysis Keep the default file system as EMRFS Keep all other configurations unchanged Phase 3: Open-source S3A testing Replace only the hadoop-aws.jar file with the open-source Hadoop S3A 3.4.1 implementation Keep all other components configured identically This controlled test environment is critical for the evaluation for the following reasons:\nIt isolates the specific performance impact of the S3A connector implementation It eliminates hidden variables that could skew the results It provides accurate measurements of performance improvements between Amazon’s S3A implementation and the open-source alternative Benchmark execution and results Throughout the benchmark runs, we maintained consistent testing conditions and configurations, ensuring that any observed performance differences could be directly attributed to variations in the S3A connector implementation.\nA total of 104 SparkSQL queries were executed Each query was run for 10 consecutive iterations The average runtime across the 10 iterations per query was used for comparison The average runtime for the 10 iterations on Amazon EMR 7.10 runtime for Apache Spark with EMR S3A was 1116.87 seconds, which is:\n1.08× faster than the open-source S3A connector Comparable to EMRFS The following table summarizes the metrics:\nMetric OSS S3A EMRFS EMR S3A Average runtime (seconds) 1208.26 1129.64 1116.87 Geometric mean of query runtimes (sec.) 7.63 7.09 6.99 Total cost * $6.53 $6.40 $6.15 * Detailed cost estimates are discussed later in this post.\nA corresponding chart (not shown here) illustrates the per-query performance improvements of EMR S3A over the open-source S3A connector on Amazon EMR 7.10 runtime for Apache Spark:\nPerformance speedups vary across queries The maximum observed speedup is 1.51× for query q3, where Amazon EMR S3A outperforms the open-source S3A The x-axis orders TPC-DS 3 TB benchmark queries by decreasing performance improvement The y-axis represents the speedup factor Read cost comparison Our benchmark reports produced both total runtime and the geometric mean of query runtimes to measure Spark runtime performance. Adding a cost metric provides additional insight.\nCost estimates were computed using the following formulas. These include costs for:\nAmazon EC2 Amazon Elastic Block Store (Amazon EBS) Amazon EMR Amazon S3 GET and PUT request costs are not included.\nAmazon EC2 cost (including SSD) = number of instances × r5d.4xlarge hourly rate × job runtime (hours) r5d.4xlarge hourly rate = $1.152 per hour Root Amazon EBS cost = number of instances × Amazon EBS per GB-hourly rate × root EBS size × job runtime (hours) Amazon EMR cost = number of instances × r5d.4xlarge Amazon EMR cost × job runtime (hours) r5d.4xlarge Amazon EMR cost = $0.27 per hour Total cost = Amazon EC2 cost + Root Amazon EBS cost + Amazon EMR cost "},{"uri":"https://trucfptu.github.io/internshiop-report/3-blogstranslated/3.4-blog4/","title":"Blog 3","tags":[],"description":"","content":"Optimizing Amazon EMR runtime for Apache Spark with EMR S3A By Giovanni Matteo Fumarola, Narayanan Venkateswaran, Syed Shameerur Rahman, Sushil Kumar Shivashankar, and Rajarshi Sarkar – September 24, 2025 – in Amazon EMR, Analytics, Best Practices, Intermediate (200), Permalink\nWith the Amazon EMR 7.10 runtime, Amazon EMR introduced EMR S3A, an enhanced implementation of the open source S3A file system connector. This enhanced connector is now automatically configured as the default S3 file system connector for Amazon EMR deployment options, including Amazon EMR on EC2, Amazon EMR Serverless, Amazon EMR on Amazon EKS, and Amazon EMR on AWS Outposts, while maintaining full compatibility with the open source Apache Spark APIs.\nIn the Amazon EMR 7.10 runtime for Apache Spark, the EMR S3A connector delivers read performance comparable to EMRFS, as demonstrated by TPC-DS query benchmarks. The most significant performance improvements appear in write operations, with a 7% improvement for static partition overwrites and a 215% improvement for dynamic partition overwrites compared to EMRFS. In this post, we present the improved read and write performance benefits when using the Amazon EMR 7.10.0 runtime for Apache Spark with EMR S3A, compared to EMRFS and the open source S3A file system connector.\nRead performance comparison To evaluate read performance, we used a test environment based on Amazon EMR runtime version 7.10.0 running Spark 3.5.5 and Hadoop 3.4.1. The test infrastructure consists of an Amazon Elastic Compute Cloud (Amazon EC2) cluster with nine r5d.4xlarge instances. The primary node has 16 vCPUs and 128 GB of memory, and the eight core nodes provide a total of 128 vCPUs and 1024 GB of memory.\nThe performance evaluation was conducted using a comprehensive benchmarking approach to ensure accurate and meaningful results. The input dataset uses a scale factor of 3 TB, with 17.7 billion records, approximately 924 GB of compressed data, partitioned in Parquet format. Setup instructions and technical details are available in a GitHub repository. We used Spark’s in-memory data catalog to store metadata for the TPC-DS databases and tables.\nTo create a fair and accurate comparison between EMR S3A, EMRFS, and open source S3A implementations, we used a three-phase testing approach:\nPhase 1: Baseline performance Establish baseline performance using the default Amazon EMR configuration with the EMR S3A connector Create a reference point for subsequent comparisons Phase 2: EMRFS analysis Keep the default file system as EMRFS Keep all other configurations unchanged Phase 3: Open source S3A testing Replace the hadoop-aws.jar file with the open source Hadoop S3A 3.4.1 implementation Keep all other component configurations identical This controlled testing environment is crucial for the following reasons:\nIsolates the specific performance impact of the S3A connector implementation Eliminates hidden variables that could skew results Provides accurate measurements of performance improvements between the Amazon S3A implementation and the open source alternative Test execution and results Throughout the tests, we kept the test conditions and configurations consistent so that any observed performance differences could be attributed directly to variations in the S3A connector implementation.\nWe executed a total of 104 SparkSQL queries, each run 10 times. The average runtime over the 10 iterations for each query was used for comparison. The average runtime over 10 iterations on Amazon EMR 7.10 runtime for Apache Spark with EMR S3A was 1116.87 seconds, which is 1.08x faster than open source S3A and comparable to EMRFS. The figure (not shown here) illustrates the total runtime in seconds.\nThe following table summarizes the metrics:\nMetric OSS S3A EMRFS EMR S3A Average runtime (seconds) 1208.26 1129.64 1116.87 Geometric mean of queries (seconds) 7.63 7.09 6.99 Total cost * $6.53 $6.40 $6.15 * Detailed cost estimates are discussed later in this post.\nA chart (not shown) illustrates query-level performance improvements of EMR S3A compared to open source S3A on Amazon EMR 7.10 runtime for Apache Spark. The speedup varies by query, with the fastest achieving a 1.51x speedup for query q3, where Amazon EMR S3A outperforms open source S3A. The x-axis orders the 3 TB TPC-DS benchmark queries by decreasing performance gain observed with Amazon EMR, and the y-axis shows the magnitude of the speedup as a ratio.\nRead cost comparison Our benchmark outputs total runtime and geometric mean metrics to measure Spark runtime performance. A cost metric provides additional insight. Cost estimates are computed using the following formulas. They include Amazon EC2, Amazon Elastic Block Store (Amazon EBS), and Amazon EMR costs, but exclude Amazon Simple Storage Service (Amazon S3) GET and PUT costs.\nAmazon EC2 cost (including SSD cost)\nEC2 cost = number of instances * r5d.4xlarge hourly rate * job runtime (hours)\nr5d.4xlarge hourly rate = $1.152 per hour Root Amazon EBS cost\nRoot EBS cost = number of instances * EBS per GB-hour rate * root EBS size * job runtime (hours)\nAmazon EMR cost\nEMR cost = number of instances * r5d.4xlarge EMR cost * job runtime (hours)\nr5d.4xlarge Amazon EMR cost = $0.27 per hour Total cost\nTotal cost = EC2 cost + root EBS cost + EMR cost\nThe following table summarizes the cost comparison:\nMetric EMRFS EMR S3A OSS S3A Runtime (hours) 0.50 0.48 0.51 Number of EC2 instances 9 9 9 Root Amazon EBS size 0 GB 0 GB 0 GB Amazon EC2 cost $5.18 $4.98 $5.29 Amazon EBS cost $0.00 $0.00 $0.00 Amazon EMR cost $1.22 $1.17 $1.24 Total cost $6.40 $6.15 $6.53 Cost savings vs. baseline Base EMR S3A is 1.04x faster than EMRFS EMR S3A is 1.06x faster than OSS S3A Write performance comparison We also benchmarked write performance for the Amazon EMR 7.10 runtime for Apache Spark.\nStatic table/partition overwrite We evaluate static table/partition overwrite write performance across different file systems using the following Spark SQL INSERT OVERWRITE query. The SELECT * FROM range(...) clause generates data at runtime. This produces approximately 15 GB of data across exactly 100 Parquet files in Amazon S3.\nSET rows=4e9; -- 4 Billion SET partitions=100; INSERT OVERWRITE DIRECTORY \u0026#39;s3://${bucket}/perf-test/${trial_id}\u0026#39; USING PARQUET SELECT * FROM range(0, ${rows}, 1, ${partitions}); "},{"uri":"https://trucfptu.github.io/internshiop-report/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: “GenAI-powered App-DB Modernization workshop” Event Objectives Introduce how Generative AI is transforming the software development lifecycle.\nShowcase Amazon Q Developer’s capabilities in coding, testing, documentation, and architecture support.\nDemonstrate how Kiro enhances DevOps and operations through log analysis, troubleshooting, and automation.\nPresent the AI-Driven Development model and how teams can integrate AI into daily workflows.\nProvide practical demos to help participants understand how to apply these tools effectively.\nSpeakers Toan Huynh –PMP, Senior Solutions Architect (AWS) My Nguyen – Senior Solutions Architect | Applied AI Specialist | Thought Leader Key Highlights Understanding limitations such as scalability issues, high maintenance cost, tight coupling, and slow deployment cycles.\nLimited Scalability – Hard to scale to meet growing user demand.\nHigh Maintenance Costs – Legacy systems require more resources and effort to maintain.\nTight Coupling – Components are often interdependent, making updates risky.\nIdentifying the drawbacks of legacy application architecture Long product release cycles → Lost revenue/missed opportunities Inefficient operations → Reduced productivity, higher costs Non-compliance with security regulations → Security breaches, loss of reputation Transitioning to modern application architecture – Microservices Migrating to a modular system — each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handles asynchronous tasks reliably.Ensures smooth communication between services without blocking. Caching Strategy: Optimizes performance and reduces latency.Reduces repeated computations and database load. Message Handling: Enables flexible, event-driven communication. Supports pub/sub, point-to-point, and streaming patterns. Domain-Driven Design (DDD) – Workshop Notes Overview:\nDDD helps align software design with business needs, reducing complexity in large systems.\nFocused on applying practical techniques during modernization using AI tools like Amazon Q Developer.\nFour-Step Method (As presented in the workshop):\nIdentify Domain Events – Capture key business events that drive the system. Arrange Timeline – Map events in chronological order to understand flow. Identify Actors – Determine users or systems interacting with these events. Define Bounded Contexts – Separate the system into independent domains for clarity and scalability. Workshop Case Study:\nBookstore Example: Demonstrated real-world DDD application. Domain events: “Order Placed”, “Payment Completed”, “Book Shipped”. Bounded contexts: Order Management, Payment Processing, Inventory. Context Mapping:\n7 patterns introduced for integrating bounded contexts: Shared Kernel, Customer-Supplier, Conformist, Anticorruption Layer, Open Host Service, Published Language, Separate Ways. Key Takeaways:\nUsing DDD reduces tight coupling and improves system clarity. Amazon Q Developer can assist in modeling, generating code, and documentation for bounded contexts. Context mapping ensures smooth communication between independent services during modernization. Event-Driven Architecture Integration patterns: Publish/Subscribe, Point-to-Point, Streaming Benefits: Loose coupling, scalable, resilient Sync vs Async: Trade-offs for performance and reliability Compute Evolution Shared Responsibility: EC2 → ECS → Fargate → Lambda Serverless: No server management, auto-scaling, pay-per-use Choosing Compute: Functions vs Containers based on workload Amazon Q Developer Automates SDLC tasks: planning → coding → testing → deployment → maintenance Supports code transformation: Java upgrades, .NET modernization Works with AWS Transform agents: VMware, Mainframe, .NET migrations Key Takeaways Design Mindset Business-first approach: Start from business needs Ubiquitous language: Shared vocabulary between business \u0026amp; tech Bounded contexts: Manage complexity in large systems Technical Architecture Event storming: Model business processes effectively Use event-driven communication to reduce tight coupling Integration patterns: Apply sync, async, pub/sub, streaming as needed Compute selection: Decide between VM, containers, or serverless based on use case Modernization Strategy Phased Approach: Follow a clear roadmap, avoid rushing 7Rs Framework: Multiple modernization paths depending on application ROI Measurement: Focus on cost reduction and business agility Applying to Work Apply DDD: Event storming sessions with business teams Refactor Microservices: Use bounded contexts to define service boundaries Implement Event-Driven Patterns: Replace some synchronous calls with async messaging Adopt Serverless: Pilot AWS Lambda where suitable Try Amazon Q Developer: Integrate into dev workflow to boost productivity Event Experience Learning from Speakers\nGained best practices in modern application design from AWS and tech experts Understood real-world application of DDD and Event-Driven Architecture Hands-On Exposure\nEvent storming sessions helped model business processes into domain events Learned splitting microservices and defining bounded contexts Explored sync vs async communication patterns, including pub/sub, point-to-point, streaming Leveraging Tools\nExplored Amazon Q Developer for SDLC automation from planning to maintenance Automated code transformation and piloted serverless with AWS Lambda Networking and Discussions Opportunities to exchange ideas with experts, peers, and business teams Enhanced ubiquitous language between business and tech Real-world examples highlighted the business-first approach Lessons Learned DDD + Event-Driven patterns reduce coupling and improve scalability and resilience Modernization requires a phased approach with ROI measurement; rushing is risky AI tools like Amazon Q Developer can boost productivity when integrated into workflows Event Photos Add your event photos here\nOverall, the workshop provided technical knowledge and reshaped thinking on application design, system modernization, and cross-team collaboration.\n"},{"uri":"https://trucfptu.github.io/internshiop-report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Doan Trung Truc\nPhone Number: 0367457851\nEmail: trucdtse180730@fpt.edu.vn\nUniversity: FPT University Ho Chi Minh City\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 08/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://trucfptu.github.io/internshiop-report/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Introduction Elastic Beanstalk (EB) is an application management service (Platform as a Service – PaaS) provided by AWS, enabling developers to deploy and operate web applications without directly managing server infrastructure.\nIn this workshop, a Spring Boot REST API application will be deployed on Elastic Beanstalk using the Single Instance configuration and the Default VPC, ensuring simplicity, ease of deployment, and suitability for learning, experimentation, and technical demonstration.\nThe deployment process focuses on the following core steps:\nPackaging the Spring Boot application into an executable .jar file. Creating an Application and Environment on Elastic Beanstalk. Uploading and deploying the application version. Testing the service using Postman. Monitoring system logs to evaluate application behavior and troubleshoot issues. Through this workshop, participants will understand the standardized process of deploying a Java backend service on AWS, as well as how Elastic Beanstalk manages the application lifecycle in a Single Instance environment.\nThis also serves as a foundation for expanding into more advanced scenarios such as Load Balanced Environments, integrating RDS, or implementing CI/CD in future workshops.\n"},{"uri":"https://trucfptu.github.io/internshiop-report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Understand the objectives of the AWS internship. Become familiar with the working environment. Learn an overview of Cloud \u0026amp; AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference 1 Internship orientation, meet mentor \u0026amp; team 08/09/2025 08/09/2025 2 Learn about the FCJ program, read policies 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 3 Review Cloud knowledge: IaaS/PaaS/SaaS, Scalability 10/09/2025 10/09/2025 4 Learn AWS Global Infrastructure 11/09/2025 11/09/2025 https://cloudjourney.awsstudygroup.com/ 5 Overview of AWS Well-Architected Framework 12/09/2025 12/09/2025 6 Summarize knowledge — prepare for next week: AWS Services 13/09/2025 13/09/2025 Week 1 Achievements: Understood internship objectives and expectations. Gained overview of Cloud Computing \u0026amp; AWS Global Infrastructure. Learned the 6 pillars of the AWS Well-Architected Framework. Ready for Week 2: Basic AWS Services. "},{"uri":"https://trucfptu.github.io/internshiop-report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Doing task A\u0026hellip;\nWeek 3: Doing task B\u0026hellip;\nWeek 4: Doing task C\u0026hellip;\nWeek 5: Doing task D\u0026hellip;\nWeek 6: Doing task E\u0026hellip;\nWeek 7: Doing task G\u0026hellip;\nWeek 8: Doing task H\u0026hellip;\nWeek 9: Doing task I\u0026hellip;\nWeek 10: Doing task L\u0026hellip;\nWeek 11: Doing task M\u0026hellip;\nWeek 12: Doing task N\u0026hellip;\n"},{"uri":"https://trucfptu.github.io/internshiop-report/4-eventparticipated/4.5-event5/","title":"Event5","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: “AWS Well-Architected – Security Pillar Workshop” Event Objectives Understand the five pillars of the AWS Security Framework Learn best practices for IAM, detection, data protection, and incident response Explore prevention strategies for cloud threats commonly seen in Vietnam Practice real-world simulations including IAM policy validation and detection workflows Event Details Date: 29/11/2025 (Morning) Time: 08:30 AM – 12:00 PM Location: AWS Vietnam Office Security Foundation Importance of the Security Pillar in AWS Well-Architected Core principles: Least Privilege – Zero Trust – Defense in Depth Deep dive into the Shared Responsibility Model Review of common cloud threats in Vietnam (data exposure, weak IAM, misconfigurations) Pillar 1 — Identity \u0026amp; Access Management (IAM) Modern IAM Architecture (08:50 – 09:30) Avoid long-term credentials → adopt IAM Roles, federation, and short-lived tokens Use IAM Identity Center for SSO and centralized permission sets Guard multi-account security with SCP and permission boundaries Strengthen authentication with MFA, credential rotation, Access Analyzer Mini Demo: Validate IAM policy \u0026amp; simulate access evaluation Key learning:\n→ IAM is the first line of defense, and misconfiguration is the #1 root cause of cloud breaches.\nPillar 2 — Detection \u0026amp; Monitoring Continuous Monitoring (09:30 – 09:55) AWS-native detection tools: CloudTrail, GuardDuty, Security Hub Enable logging across all layers: VPC Flow Logs, ALB Logs, S3 Access Logs Event-driven alerting pipelines using EventBridge “Detection-as-Code”: maintain detection rules with IaC + version control Key learning:\n→ Security works only if you monitor everything continuously and consistently.\nPillar 3 — Infrastructure Protection Network \u0026amp; Workload Security (10:10 – 10:40) VPC segmentation: private vs public subnet placement Security Groups vs NACLs: when to use each model Advanced protection: AWS WAF, Shield, Network Firewall Workload coverage: EC2 hardening, EKS/ECS baseline security Key learning:\n→ Infrastructure must adopt multi-layer protection, not just SG/NACL.\nPillar 4 — Data Protection Encryption, Keys \u0026amp; Secrets (10:40 – 11:10) AWS KMS: key policies, grants, automatic rotation Encryption at rest and in transit: S3, EBS, RDS, DynamoDB Secrets management with Secrets Manager \u0026amp; Parameter Store Data classification and guardrails for controlled access Key learning:\n→ Encryption is not optional; effective key management is equally important.\nPillar 5 — Incident Response IR Playbook \u0026amp; Automation (11:10 – 11:40) Incident lifecycle following AWS best practices Hands-on playbooks: Compromised IAM access key S3 public exposure EC2 malware detection Isolation workflow: snapshots, quarantining, evidence collection Automated response using Lambda and Step Functions Key learning:\n→ IR must be prepared before incidents, not built during the crisis.\nKey Takeaways Security Mindset Prevention \u0026gt; Remediation — eliminate long-lived credentials, enforce Zero Trust Avoid internet-facing resources except when absolutely necessary Everything should be defined and enforced using Infrastructure as Code Technical Knowledge Understand how detection sources (CloudTrail, DNS logs, VPC Flow Logs) form a complete picture Learn structured IR workflow instead of reacting ad-hoc Network Firewall + DNS Firewall help block malicious traffic proactively Managed threat signatures + GuardDuty = continuous threat intelligence Practical Skills Validating IAM policies with Access Analyzer Setting up org-level CloudTrail with EventBridge Using S3 Block Public Access effectively Applying encryption policies and secret rotation patterns Applying to Work Implement short-lived credentials and enforce MFA across all accounts Introduce GuardDuty, Security Hub, and centralized logging Review VPC architecture to limit public exposure Apply encryption standards consistently (S3, RDS, EBS, DynamoDB) Build simple IR runbooks for common incidents in the environment Automate alert → containment → remediation using Lambda Event Experience Attending the AWS Security Pillar Workshop helped reinforce both theoretical and practical aspects of cloud security.\nInsights from speakers The speakers highlighted how security failures often stem from misconfigurations, not from AWS itself. Their real-world examples clarified the importance of least privilege, network segmentation, and continuous monitoring. Practical sessions Running IAM access simulations gave me a clearer understanding of effective permission boundaries. The walkthrough of S3 exposure scenarios helped me understand how quickly data can leak without proper guardrails. Learning about AWS Network Firewall and DNS Firewall opened a new view on proactive threat blocking. Tools \u0026amp; automation GuardDuty findings, Active Threat Defense, and automated remediation workflows showed how security can scale across multi-account environments. I saw how IaC and “Detection-as-Code” can make security predictable and reproducible. Discussion \u0026amp; networking Discussions with AWS experts and participants helped me understand common security mistakes among Vietnamese companies. Conversations reinforced that security is a continuous process, not a one-time effort. Lessons learned Zero Trust + Least Privilege are non-negotiable. Automate everything: logging, detection, incident response. Prevention reduces 80% of potential incidents. Build IR playbooks before emergencies. Some event photos Add your event photos here\nThe workshop reshaped the way I think about cloud security—from reactive protection to proactive, automated, and continuously improving defense.\n"},{"uri":"https://trucfptu.github.io/internshiop-report/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"Prepare and create the build file (.jar) Before deploying to AWS, the Spring Boot application needs to be packaged into an executable .jar file.\nIn the project’s root directory, run the command:\nmvn clean package -DskipTests After the process completes, the built file will appear in the directory:\n/target/\u0026lt;application-name\u0026gt;.jar "},{"uri":"https://trucfptu.github.io/internshiop-report/2-proposal/","title":"Proposal","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nIn this section, you need to summarize the contents of the workshop that you plan to conduct.\nSportShop E-Commerce Platform A Cost-Optimized AWS Three-Tier Architecture for Online Sports Retail 1. Executive Summary The SportShop E-Commerce Platform is designed to modernize online retail operations using a simplified and cost-optimized AWS architecture, suitable for student projects and small-scale applications. The system uses ReactJS for the frontend, Spring Boot for the backend, and Amazon RDS MySQL for persistent data storage.\nTo reduce cost and complexity, the architecture removes Amazon Cognito, the Application Load Balancer (ALB), NAT Gateway, and banking payment integration. User authentication is handled directly by the backend, which runs on Elastic Beanstalk single-instance mode, while static content is delivered through S3 + CloudFront.\nDespite being simplified, the architecture still follows AWS best practices for security and CI/CD using ACM, Parameter Store, CloudWatch, CodePipeline, and CodeBuild.\n2. Problem Statement Current Problems Slow, manual, and inconsistent deployment workflows Risk of leaked or improperly stored credentials Limited system monitoring and alerting High chance of downtime during updates Proposed Solution The platform leverages AWS managed services to automate deployments, enhance security, and implement a clean three-layer architecture:\nEdge: Route 53, CloudFront (ACM) Application: Elastic Beanstalk (single-instance) Data: RDS MySQL, S3, CloudWatch Secrets such as database credentials and JWT signing keys are securely stored in Parameter Store.\nCI/CD is managed through GitLab → CodePipeline → CodeBuild.\nBenefits Lower AWS cost and reduced architectural complexity Faster deployments through automation Global HTTPS delivery through CloudFront Clear separation between frontend, backend, and database layers 3. Solution Architecture AWS Services Used Amazon Route 53 — DNS Amazon CloudFront — CDN and HTTPS distribution AWS Certificate Manager — SSL/TLS certificates Elastic Beanstalk (Single Instance) — Backend hosting Amazon RDS MySQL — Main database Amazon S3 — Static hosting \u0026amp; media storage Parameter Store — Secure secret management CloudWatch — Logs and metrics CodePipeline + CodeBuild — CI/CD automation Layered Architecture Edge Layer\nRoute 53 → CloudFront CloudFront serves the ReactJS frontend globally over HTTPS Application Layer\nSpring Boot backend deployed on Elastic Beanstalk single-instance Authentication handled with JWT Secure configuration loaded from Parameter Store Data Layer\nRDS MySQL stores structured application data S3 stores static content and media files CloudWatch provides centralized monitoring 4. Technical Implementation Configure VPC, subnets, and security groups Host frontend on S3 + CloudFront Deploy backend with Elastic Beanstalk Provision RDS MySQL Store secrets in Parameter Store CI/CD pipeline: GitLab → CodePipeline → CodeBuild Enable monitoring via CloudWatch Tech Stack:\nFrontend: ReactJS (S3 + CloudFront) Backend: Spring Boot (Java 17) Database: Amazon RDS MySQL CI/CD: AWS CodePipeline, CodeBuild Security: IAM, ACM, Parameter Store, CloudWatch 5. Timeline \u0026amp; Milestones Month 1 Set up AWS environment (VPC, RDS, S3, CloudFront, Route 53) Deploy backend \u0026amp; frontend Configure CI/CD pipeline Month 2 Implement JWT authentication Develop modules: product, order, user management Set up monitoring \u0026amp; alerting Month 3 Testing and optimization Final deployment Documentation and training 6. Budget Estimation You can see the cost on the AWS Pricing Calculator Or download the budget estimate file.\nAWS Service Monthly Cost Amazon RDS MySQL $21.84 Elastic Beanstalk (EC2 t3.micro) $11.68 Amazon S3 $0.26 Data Transfer $0.60 CloudFront $0.88 Route 53 $0.90 ACM Public Certificate $0 CloudWatch Metrics $3.02 CodePipeline $0 CodeBuild $0.50 Parameter Store $0 CloudWatch Logs $1.41 Estimated total: ~$35–40 per month\nYearly: ~$420–480\n7. Risk Assessment Risks Single-instance backend → No fault tolerance No NAT Gateway → Backend cannot call external APIs Risk of JWT token misuse without proper security Mitigation Enable Elastic Beanstalk auto-recovery and health checks Apply secure authentication \u0026amp; token handling practices Utilize RDS automated backups and snapshots 8. Future Enhancements Amazon Cognito — MFA, OAuth, Social Login Application Load Balancer — Multi-instance scaling \u0026amp; high availability NAT Gateway — Allow backend to call external APIs Payment Integrations: VNPay, Stripe, PayPal RDS Multi-AZ — High availability and automatic failover S3 Lifecycle + Glacier — Cost-optimized long-term storage AWS X-Ray + OpenSearch — Tracing and advanced logging Microservices Architecture with ECS/EKS — Scale backend into multiple services "},{"uri":"https://trucfptu.github.io/internshiop-report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Get familiar with basic AWS Services. Use AWS Console \u0026amp; CLI. Tasks: Day Task Start End 1 Get familiar with FCJ team, read requirements 15/09/2025 15/09/2025 2 Learn AWS service groups: Compute, Storage 16/09/2025 16/09/2025 3 Create AWS Free Tier Account 17/09/2025 17/09/2025 4 Practice AWS CLI 18/09/2025 18/09/2025 5 Learn basic EC2 19/09/2025 19/09/2025 6 Practice EC2 SSH + EBS 20/09/2025 20/09/2025 Achievements: Created and configured an AWS account. Used AWS CLI \u0026amp; Console. Able to launch EC2 and work with EBS. "},{"uri":"https://trucfptu.github.io/internshiop-report/5-workshop/5.3-create-environmnet/","title":"Create environment","tags":[],"description":"","content":"Create Elastic Beanstalk Application (Default VPC) Log in to the AWS Console and open the Elastic Beanstalk service.\nProceed to create a new Application with the following settings:\nApplication name: set according to the project name Platform: Java (Corretto) – matching the JDK version used by the application Environment type: Web Server VPC: use the Default VPC (Elastic Beanstalk automatically detects and selects it) Environment tier: Single Instance (no Load Balancer) Using the Default VPC simplifies the deployment process,\neliminating the need to manually configure subnets, route tables, or security groups.\n"},{"uri":"https://trucfptu.github.io/internshiop-report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs you have translated. For example:\nBlog 1 - AWS Site-to-Site VPN Now Supports IPv6 on External IP Addresses This blog introduces the newly added capability that allows AWS Site-to-Site VPN tunnels to use IPv6 addresses for their external endpoints. This enhancement enables customers to build fully native IPv6 networking without requiring IPv6-to-IPv4 translation, which simplifies network modernization and supports IPv6-only architectures. You will learn why IPv6 adoption is growing, how it helps conserve scarce IPv4 space, reduces networking costs, and helps regulated industries meet IPv6 compliance requirements.\nThe blog also walks you through deploying a simulated on-premises environment and AWS infrastructure using CloudFormation, configuring an IPv6 VPN connection via Transit Gateway, understanding route propagation behavior, and validating tunnel connectivity in a real-world hybrid network setup.\nBlog 2 - Optimizing the Amazon EMR Runtime for Apache Spark with EMR S3A This blog explains how Amazon EMR 7.10 significantly improves Apache Spark read and write performance using EMR S3A—an enhanced version of the open-source S3A connector now enabled by default across EMR on EC2, Serverless, EKS, and Outposts. You will learn how EMR S3A provides EMRFS-comparable read performance while delivering major write performance gains, including 7% improvements for static partition overwrites and up to 215% for dynamic partition overwrites.\nThe article presents the benchmarking architecture (Spark 3.5.5, Hadoop 3.4.1, r5d.4xlarge cluster), the three-phase evaluation methodology (baseline EMR S3A, EMRFS testing, and OSS S3A comparison), and results from executing 104 SparkSQL queries. It also includes cost comparisons covering Amazon EC2, EBS, and EMR charges to give a complete view of performance and cost efficiency when using EMR S3A over EMRFS and the open-source S3A implementation.\nBlog 3 - Cross-account Disaster Recovery with AWS Elastic Disaster Recovery in Secure Networks (Part 1) This blog is the first part of a two-part series that demonstrates how to build a secure, cross-account disaster recovery (DR) architecture using AWS Elastic Disaster Recovery (DRS). It focuses on the network architecture and required configurations to maintain network isolation, preserve IP addressing schemes, and ensure application continuity during failover and failback operations.\nThe article explains how AWS PrivateLink, VPC Peering, and Route 53 private hosted zones enable secure, private connectivity to AWS services without exposing traffic to the public internet—making it suitable for highly regulated or sensitive environments. The reference architecture spans production and recovery accounts across two AWS Regions (Ireland and London), each with production, staging, and recovery VPCs. It also details how to configure VPC endpoints for Elastic Disaster Recovery, Amazon S3, AWS STS, and Amazon EC2 to support replication, failover, and reverse replication. This foundation enables resilient, secure DR workflows for mission-critical workloads.\n"},{"uri":"https://trucfptu.github.io/internshiop-report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Deep dive into EC2 \u0026amp; Storage. Practice using S3. Tasks: Day Task Start End 1 Review advanced EC2 concepts 22/09/2025 22/09/2025 2 Learn Security Groups and Key Pairs 23/09/2025 23/09/2025 3 Learn EBS and Snapshots 24/09/2025 24/09/2025 4 Introduction to S3 25/09/2025 25/09/2025 5 Hands-on: S3 Bucket 26/09/2025 26/09/2025 6 Summary 27/09/2025 27/09/2025 Achievements: Understood advanced EC2 architecture. Able to perform basic S3 operations. "},{"uri":"https://trucfptu.github.io/internshiop-report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" In this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://trucfptu.github.io/internshiop-report/5-workshop/5.4upload-and-deploy/","title":"Upload and deploy","tags":[],"description":"","content":"Upload the .jar file and deploy the application In the newly created Environment screen → select Upload and Deploy.\nProceed with:\nSelecting the built .jar file\nSetting a Version label (e.g., v1-initial)\nConfirming the deployment\nElastic Beanstalk will automatically:\nUpload the file to S3 Launch a single EC2 instance Start the Spring Boot application in the selected Java environment "},{"uri":"https://trucfptu.github.io/internshiop-report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Learn RDS and DynamoDB. Understand relational vs non-relational databases. Practice launching and operating AWS database services. Tasks: Day Task Start End 1 Learn RDS basics 28/09/2025 28/09/2025 2 Launch an RDS instance (MySQL/PostgreSQL) 29/09/2025 29/09/2025 3 Learn DynamoDB fundamentals 30/09/2025 30/09/2025 4 Perform CRUD operations on DynamoDB 01/10/2025 01/10/2025 5 Compare SQL vs NoSQL, use cases 02/10/2025 02/10/2025 6 Weekly summary \u0026amp; documentation 03/10/2025 03/10/2025 Achievements: Understood the core concepts of RDS. Successfully launched and configured an RDS database instance. Learned DynamoDB structure and performed CRUD operations. Clearly understood differences between SQL and NoSQL. "},{"uri":"https://trucfptu.github.io/internshiop-report/5-workshop/5.5-policy/","title":"Access the application and test","tags":[],"description":"","content":"Access the application and test the API using Postman After the deployment succeeds, Elastic Beanstalk provides a URL in the format:\nhttp://\u0026lt;environment-id\u0026gt;.\u0026lt;region\u0026gt;.elasticbeanstalk.com Use this URL to send requests from Postman to the Spring Boot API.\nExample:\nGET http://\u0026lt;env\u0026gt;.elasticbeanstalk.com/api/hello If the API responds with the expected data, the deployment is considered successful.\n"},{"uri":"https://trucfptu.github.io/internshiop-report/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Deploying a Spring Boot REST API application to AWS Elastic Beanstalk Overview This workshop demonstrates the process of deploying a Spring Boot REST API application to Elastic Beanstalk in Single Instance mode.\nThis deployment model does not use a Load Balancer, does not configure Auto Scaling, and runs entirely within the default VPC of the AWS account.\nThe goals of this workshop include: Building the .jar file of the Spring Boot project. Creating a simple Elastic Beanstalk Application and Environment. Uploading and deploying the .jar file to the environment. Testing the API using Postman. Observing and analyzing logs through request logs. Content Workshop overview Prerequiste Create environment Upload and Deploy Access the application and test Monitoring and Clean up "},{"uri":"https://trucfptu.github.io/internshiop-report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Learn IAM deeply. Understand best practices for security. Practice creating IAM users, roles, and policies. Tasks: Day Task Start End 1 Review IAM basics 06/10/2025 06/10/2025 2 Learn IAM Users \u0026amp; Groups 07/10/2025 07/10/2025 3 Learn IAM Policies \u0026amp; Permissions 08/10/2025 08/10/2025 4 Learn IAM Roles, STS 09/10/2025 09/10/2025 5 Hands-on: create custom IAM policies 10/10/2025 10/10/2025 6 Summary \u0026amp; review 11/10/2025 11/10/2025 Achievements: Understood IAM best practices. Able to create fine-grained IAM policies. Learned how to manage roles and temporary credentials. "},{"uri":"https://trucfptu.github.io/internshiop-report/5-workshop/5.6-cleanup/","title":"Monitoring and Clean up","tags":[],"description":"","content":"Monitoring deployment logs and application logs Elastic Beanstalk provides two methods for viewing logs:\n1. Request Logs in Elastic Beanstalk Accessible via Logs → Request logs → Last 100 lines.\nThis log shows:\nThe Spring Boot startup process Error logs if the application fails Requests/responses processed by the runtime environment 2, CloudWatch Logs Elastic Beanstalk automatically pushes certain logs to CloudWatch,\nespecially system logs from the EC2 instance.\nClean up resources after deployment To avoid unexpected costs, it is recommended to delete the resources created after completing the workshop.\n1. Delete the Environment Go to Elastic Beanstalk → select the environment → Actions → Terminate environment.\nThis will remove the EC2 instance, the auto-generated security groups, and related resources.\n2. Delete the Application (if no longer needed) Elastic Beanstalk → Application → Actions → Delete Application.\n3. Delete files in S3 Elastic Beanstalk stores .jar versions inside an S3 bucket.\nGo to S3 → locate the bucket created by EB → delete the unnecessary versions.\n4. Review CloudWatch Logs You may delete the log groups if you want to clean up further.\n"},{"uri":"https://trucfptu.github.io/internshiop-report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my work and study, I always try to learn and do my job well, but there are also unexpected events that make me delay and lose my spirit. After a period of time at Amazon Web Service, I gained experience, learned and met enthusiastic and talented people who helped me improve myself.\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ✅ ☐ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ✅ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ☐ ✅ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ✅ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ☐ ✅ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ☐ ✅ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ☐ ✅ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ✅ ☐ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ✅ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Needs Improvement Enhance self-study ability, discipline in learning and researching Build reasonable working time, have a delay deadline Upgrade the next ability and problem solving and improve foreign languages "},{"uri":"https://trucfptu.github.io/internshiop-report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Learn VPC in depth. Understand networking components: Subnet, Routing, NAT, etc. Tasks: Day Task Start End 1 Learn VPC basics 13/10/2025 13/10/2025 2 Learn Route Table, Internet Gateway 14/10/2025 14/10/2025 3 Learn NAT Gateway, NACL 15/10/2025 15/10/2025 4 Learn Security Groups (advanced) 16/10/2025 16/10/2025 5 Hands-on: create a custom VPC 17/10/2025 17/10/2025 6 Summary 18/10/2025 18/10/2025 Achievements: Built a custom VPC with public/private subnets. Understood routing, NAT, and network security models. "},{"uri":"https://trucfptu.github.io/internshiop-report/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nFriendly and good working environment. FCJ members are enthusiastic, always ready to help me whenever I have questions and learn. Comfortable and tidy working space. I hope that in the future there will be exchanges or outside activities to help connect more easily.\n2. Learning \u0026amp; Skill Development Opportunities\nDuring my internship, I learned a lot of great services and tools, developed my discipline, got familiar with the working space, improved my communication and teamwork skills. There were many opportunities to listen to great instructors and events sharing their experiences and valuable lessons that helped me orient my future well.\n3. Internship Policies / Benefits\nThe company offers flexible working hours, the opportunity to attend events and training sessions, and to listen to talented and enthusiastic people who help improve and understand more about the tools and answer interns\u0026rsquo; questions.\n4. Mentor and support Team The instructor is enthusiastic, dedicated and experienced. The support team always tries and is ready to help, create favorable conditions and a comfortable working environment, and solve problems and serious questions.\nAdditional Questions What did you find most satisfying during your internship?\n-Working environment, services, tools and knowledge and experience sharing sessions\nIf recommending to a friend, would you suggest they intern here? Why or why not?\nIf I introduce my friends to do internship here, I will advise them to be confident in communicating, learning more, and being proactive in studying and arranging their own time.\nBecause in the company there are many opportunities and knowledge and there are wonderful people, enthusiastic about work, continuous learning from each other. Internship at the company you will be surprised about the working environment as well as your instructors.\n"},{"uri":"https://trucfptu.github.io/internshiop-report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Learn about relational databases in AWS. Practice using RDS. Tasks: Day Task Start End 1 Introduction to RDS 20/10/2025 20/10/2025 2 Learn DB engines, storage types 21/10/2025 21/10/2025 3 Multi-AZ \u0026amp; Read Replicas 22/10/2025 22/10/2025 4 Backup \u0026amp; restore 23/10/2025 23/10/2025 5 Hands-on: create RDS instance 24/10/2025 24/10/2025 6 Summary 25/10/2025 25/10/2025 Achievements: Created and configured an RDS database. Understood replication, failover, and backups. "},{"uri":"https://trucfptu.github.io/internshiop-report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Learn serverless concepts. Practice using AWS Lambda and API Gateway. Tasks: Day Task Start End 1 Learn Serverless Architecture 27/10/2025 27/10/2025 2 Learn AWS Lambda fundamentals 28/10/2025 28/10/2025 3 Practice Lambda functions 29/10/2025 29/10/2025 4 Learn API Gateway 30/10/2025 30/10/2025 5 Integrate Lambda + API Gateway 31/10/2025 31/10/2025 6 Summary 01/11/2025 01/11/2025 Achievements: Built a serverless API with Lambda and API Gateway. "},{"uri":"https://trucfptu.github.io/internshiop-report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Learn S3 advanced. Practice S3 static website hosting. Tasks: Day Task Start End 1 S3 storage classes 03/11/2025 03/11/2025 2 Lifecycle rules 04/11/2025 04/11/2025 3 S3 versioning 05/11/2025 05/11/2025 4 S3 static website hosting 06/11/2025 06/11/2025 5 Configure CloudFront for S3 07/11/2025 07/11/2025 6 Summary 08/11/2025 08/11/2025 Achievements: Hosted a static website on S3 + CloudFront. "},{"uri":"https://trucfptu.github.io/internshiop-report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Learn monitoring \u0026amp; logging. Practice CloudWatch and CloudTrail. Tasks: Day Task Start End 1 Learn CloudWatch Metrics 10/11/2025 10/11/2025 2 Learn CloudWatch Logs 11/11/2025 11/11/2025 3 CloudWatch Alarms 12/11/2025 12/11/2025 4 Learn CloudTrail 13/11/2025 13/11/2025 5 Hands-on: monitor EC2 14/11/2025 14/11/2025 6 Summary 15/11/2025 15/11/2025 Achievements: Able to track logs, metrics, and set alarms. "},{"uri":"https://trucfptu.github.io/internshiop-report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":" ⚠️ Reference only.\nWeek 11 Objectives: Integrate CI/CD with AWS CodePipeline. Deploy an application automatically. Tasks: Day Task Start End 1 Learn CodeCommit 17/11/2025 17/11/2025 2 Learn CodeBuild 18/11/2025 18/11/2025 3 Learn CodeDeploy 19/11/2025 19/11/2025 4 Create a CI/CD Pipeline 20/11/2025 20/11/2025 5 Test pipeline automation 21/11/2025 21/11/2025 6 Summary 22/11/2025 22/11/2025 Achievements: Built a complete CI/CD pipeline. Application deploys automatically on every code push. "},{"uri":"https://trucfptu.github.io/internshiop-report/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":" ⚠️ Reference only.\nWeek 12 Objectives: Finalize internship knowledge. Complete final AWS project. Prepare internship report \u0026amp; presentation. Tasks: Day Task Start End 1 Define project scope \u0026amp; requirements 24/11/2025 24/11/2025 2 Design architecture (S3/EC2/RDS/Lambda…) 25/11/2025 25/11/2025 3 Deploy the project to AWS 26/11/2025 26/11/2025 4 Optimize cost \u0026amp; security 27/11/2025 27/11/2025 5 Write internship report \u0026amp; prepare slides 28/11/2025 28/11/2025 6 Internship summary \u0026amp; final submission 29/11/2025 29/11/2025 Achievements: Completed final AWS project. Applied knowledge from the 12-week internship. Finished report and presentation slides. "},{"uri":"https://trucfptu.github.io/internshiop-report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://trucfptu.github.io/internshiop-report/tags/","title":"Tags","tags":[],"description":"","content":""}]